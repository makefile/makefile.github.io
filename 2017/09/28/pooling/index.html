<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>深度学习网络层之 Pooling | 康行天下</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="pooling 是仿照人的视觉系统进行降维（降采样），用更高层的抽象表示图像特征，这一部分内容从Hubel&amp;amp;wiesel视觉神经研究到Fukushima提出，再到LeCun的LeNet5首次采用并使用BP进行求解，是一条线上的内容，原始推动力其实就是仿生，仿照真正的神经网络构建人工网络。至于pooling为什么可以这样做，是因为：我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”">
<meta name="keywords" content="Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习网络层之 Pooling">
<meta property="og:url" content="https://makefile.ml/2017/09/28/pooling/index.html">
<meta property="og:site_name" content="康行天下">
<meta property="og:description" content="pooling 是仿照人的视觉系统进行降维（降采样），用更高层的抽象表示图像特征，这一部分内容从Hubel&amp;amp;wiesel视觉神经研究到Fukushima提出，再到LeCun的LeNet5首次采用并使用BP进行求解，是一条线上的内容，原始推动力其实就是仿生，仿照真正的神经网络构建人工网络。至于pooling为什么可以这样做，是因为：我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://makefile.ml/img/1506524748407_2.png">
<meta property="og:updated_time" content="2018-02-02T06:46:32.886Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习网络层之 Pooling">
<meta name="twitter:description" content="pooling 是仿照人的视觉系统进行降维（降采样），用更高层的抽象表示图像特征，这一部分内容从Hubel&amp;amp;wiesel视觉神经研究到Fukushima提出，再到LeCun的LeNet5首次采用并使用BP进行求解，是一条线上的内容，原始推动力其实就是仿生，仿照真正的神经网络构建人工网络。至于pooling为什么可以这样做，是因为：我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”">
<meta name="twitter:image" content="https://makefile.ml/img/1506524748407_2.png">
  
    <link rel="alternate" href="/atom.xml" title="康行天下" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">康行天下</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">追求卓越</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://makefile.ml"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-pooling" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/09/28/pooling/" class="article-date">
  <time datetime="2017-09-28T16:52:58.000Z" itemprop="datePublished">2017-09-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深度学习网络层之 Pooling
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>pooling 是仿照人的视觉系统进行降维（降采样），用更高层的抽象表示图像特征，这一部分内容从Hubel&amp;wiesel视觉神经研究到Fukushima提出，再到LeCun的LeNet5首次采用并使用BP进行求解，是一条线上的内容，原始推动力其实就是仿生，仿照真正的神经网络构建人工网络。<br>至于pooling为什么可以这样做，是因为：我们之所以决定使用卷积后的特征是因为图像具有一种“静态性”的属性，这也就意味着在一个图像区域有用的特征极有可能在另一个区域同样适用。因此，为了描述大的图像，一个很自然的想法就是对不同位置的特征进行聚合统计。这个均值或者最大值就是一种聚合统计的方法。<br>做窗口滑动卷积的时候，卷积值就代表了整个窗口的特征。因为滑动的窗口间有大量重叠区域，出来的卷积值有冗余，进行最大pooling或者平均pooling就是减少冗余。减少冗余的同时，pooling也丢掉了局部位置信息，所以局部有微小形变，结果也是一样的。<br>pooling层通常的作用是:减少空间大小,减少网络参数,防止过拟合。</p>
</blockquote>
<h2 id="pooling-种类"><a href="#pooling-种类" class="headerlink" title="pooling 种类"></a>pooling 种类</h2><p>最常见的池化操作为最大池化和平均池化：</p>
<h3 id="最大池化-Max-Pooling"><a href="#最大池化-Max-Pooling" class="headerlink" title="最大池化 Max Pooling"></a>最大池化 Max Pooling</h3><p>  前向传播：选图像区域的最大值作为该区域池化后的值。<br>  反向传播：梯度通过最大值的位置传播，其它位置梯度为0。</p>
<h3 id="平均池化-Average-Pooling（也称mean-pooling）"><a href="#平均池化-Average-Pooling（也称mean-pooling）" class="headerlink" title="平均池化 Average Pooling（也称mean pooling）"></a>平均池化 Average Pooling（也称mean pooling）</h3><p>  前向传播：计算图像区域的平均值作为该区域池化后的值。<br>  反向传播：梯度取均值后分给每个位置。<br>对于Average Pooling的输入$X=x_1,x_2,…x<em>n$，输出$\displaystyle f(X) = \frac{1}{n} \sum</em>{i=1}^n x_i$<br>$$<br>\begin{align}<br>\displaystyle \frac{\partial f}{\partial x_j} (X)  = \frac{\partial f}{\partial x<em>j} \frac{1}{n} \sum</em>{i=1}^n x<em>i \<br>\displaystyle = \frac{1}{n} \sum</em>{i=1}^n  \frac{\partial f}{\partial x_j} x<em>i \<br>\displaystyle = \frac{1}{n} \sum</em>{i=1}^n \delta(i-j) \<br>当i=j时，δ(x)=1，否则为0.<br>\end{align}<br>$$</p>
<h3 id="Stochastic-Pooling"><a href="#Stochastic-Pooling" class="headerlink" title="Stochastic Pooling"></a>Stochastic Pooling</h3><p>论文<a href="https://arxiv.org/abs/1301.3557" target="_blank" rel="noopener">Stochastic Pooling for Regularization of Deep Convolutional Neural Networks</a>提出了一种简单有效的正则化CNN的方法,能够降低max pooling的过拟合现象，提高泛化能力。对于pooling层的输入，根据输入的多项式分布随机选择一个值作为输出。训练阶段和测试阶段的操作略有不同。</p>
<p><strong>训练阶段</strong></p>
<ol>
<li><p>前向传播</p>
<p>(1)归一化pooling的输入，作为每个激活神经元的分布概率值$p_i={a<em>i\over\sum</em>{k\in R_j}a_k}$.</p>
<p>(2)从基于$p$的多项式分布中随机采样一个位置的值作为输出。</p>
</li>
<li><p>反向传播<br>跟max pooling类似，梯度通过被选择的位置传播，其它位置为0.</p>
</li>
</ol>
<p><strong>测试阶段</strong></p>
<p>如果在测试时也使用随机pooling会对预测值引入噪音，降低性能。取而代之的是使用按归一化的概率值加权平均。比使用average pooling表现要好一些。因此在平均意义上，与average pooling近似，在局部意义上，则服从max pooling的准则。</p>
<p><strong>解释分析</strong></p>
<p>按概率加权的方式可以被看作是一种模型平均融合的方式，在pooling区域不同选择方式对应一个新模型。训练阶段由于引入随机性，所以会改变网络的连接结构，导致产生新的模型。在测试阶段会同时使用这些模型，做加权平均。假设网络有d层pooling层，pooling核大小是n，那么可能的模型有$n^d$个。这比dropout增加的模型多样性要多（dropout率为0.5时相当于n=2）。<br>在CIFAR-10上三种pooling方法的错误率对比：<br><img src="/img/1506524748407_2.png" alt="CIFAR-10-stochastic"></p>
<h2 id="pooling-选择与实际应用"><a href="#pooling-选择与实际应用" class="headerlink" title="pooling 选择与实际应用"></a>pooling 选择与实际应用</h2><p>通常我们使用Max Pooling，因为使用它能学到图像的边缘和纹理结构。而Average Pooling则不能。Max Pooling通常用以减小估计值方差，在方差不太重要的地方可以随意选择Max Pooling和Average Pooling。Average Pooling用以减小估计均值的偏移。在某些情况下Average Pooling可能取得比Max Pooling稍好一些的效果。<br>average pooling会弱化强激活值，而max pooling保留最强的激活值却容易过拟合。<br>虽然从理论上说Stochastic Pooling也许能取得较好的结果，但是需要在实践中多次尝试，随意使用可能效果变差。因此并不是一个常规的选择。<br>按池化是否作用于图像中不重合的区域（这与卷积操作不同）分为一般池化（Gerneral Pooling）与重叠池化（OverlappingPooling）。<br>常见设置是filter大小F=2,步长S=2或F=3,S=2(overlapping pooling,重叠)；pooling层通常不需要填充。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p><strong>caffe cpu版pooling层实现代码</strong><a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cpp" target="_blank" rel="noopener">pooling_layer.cpp</a>:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> PoolingLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">switch</span> (<span class="keyword">this</span>-&gt;layer_param_.pooling_param().pool()) &#123;</span><br><span class="line">  <span class="keyword">case</span> PoolingParameter_PoolMethod_MAX:</span><br><span class="line">  	  <span class="keyword">const</span> <span class="keyword">int</span> pool_index = ph * pooled_width_ + pw;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> h = hstart; h &lt; hend; ++h) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> w = wstart; w &lt; wend; ++w) &#123;</span><br><span class="line">          <span class="keyword">const</span> <span class="keyword">int</span> index = h * width_ + w;</span><br><span class="line">          <span class="keyword">if</span> (bottom_data[index] &gt; top_data[pool_index]) &#123;</span><br><span class="line">            top_data[pool_index] = bottom_data[index];</span><br><span class="line">            <span class="keyword">if</span> (use_top_mask) &#123;</span><br><span class="line">              top_mask[pool_index] = <span class="keyword">static_cast</span>&lt;Dtype&gt;(index);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              mask[pool_index] = index;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   <span class="keyword">case</span> PoolingParameter_PoolMethod_AVE:</span><br><span class="line">      ...</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; top_count; ++i) &#123;</span><br><span class="line">        top_data[i] = <span class="number">0</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> h = hstart; h &lt; hend; ++h) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> w = wstart; w &lt; wend; ++w) &#123;</span><br><span class="line">          top_data[ph * pooled_width_ + pw] +=</span><br><span class="line">            bottom_data[h * width_ + w];</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      top_data[ph * pooled_width_ + pw] /= pool_size;</span><br><span class="line">      ...</span><br><span class="line">   <span class="keyword">case</span> PoolingParameter_PoolMethod_STOCHASTIC:</span><br><span class="line">    NOT_IMPLEMENTED;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> PoolingLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line">  <span class="keyword">if</span> (!propagate_down[<span class="number">0</span>]) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">switch</span> (<span class="keyword">this</span>-&gt;layer_param_.pooling_param().pool()) &#123;</span><br><span class="line">  <span class="keyword">case</span> PoolingParameter_PoolMethod_MAX:</span><br><span class="line">    <span class="comment">// The main loop</span></span><br><span class="line">    <span class="keyword">if</span> (use_top_mask) &#123;</span><br><span class="line">      top_mask = top[<span class="number">1</span>]-&gt;cpu_data();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      mask = max_idx_.cpu_data();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> n = <span class="number">0</span>; n &lt; top[<span class="number">0</span>]-&gt;num(); ++n) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; channels_; ++c) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> ph = <span class="number">0</span>; ph &lt; pooled_height_; ++ph) &#123;</span><br><span class="line">          <span class="keyword">for</span> (<span class="keyword">int</span> pw = <span class="number">0</span>; pw &lt; pooled_width_; ++pw) &#123;</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">int</span> index = ph * pooled_width_ + pw;</span><br><span class="line">            <span class="keyword">const</span> <span class="keyword">int</span> bottom_index =</span><br><span class="line">                use_top_mask ? top_mask[index] : mask[index];</span><br><span class="line">            bottom_diff[bottom_index] += top_diff[index];</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        bottom_diff += bottom[<span class="number">0</span>]-&gt;offset(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        top_diff += top[<span class="number">0</span>]-&gt;offset(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> (use_top_mask) &#123;</span><br><span class="line">          top_mask += top[<span class="number">0</span>]-&gt;offset(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          mask += top[<span class="number">0</span>]-&gt;offset(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> PoolingParameter_PoolMethod_AVE:</span><br><span class="line">    <span class="comment">// The main loop</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> n = <span class="number">0</span>; n &lt; top[<span class="number">0</span>]-&gt;num(); ++n) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; channels_; ++c) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> ph = <span class="number">0</span>; ph &lt; pooled_height_; ++ph) &#123;</span><br><span class="line">          <span class="keyword">for</span> (<span class="keyword">int</span> pw = <span class="number">0</span>; pw &lt; pooled_width_; ++pw) &#123;</span><br><span class="line">            <span class="keyword">int</span> hstart = ph * stride_h_ - pad_h_;</span><br><span class="line">            <span class="keyword">int</span> wstart = pw * stride_w_ - pad_w_;</span><br><span class="line">            <span class="keyword">int</span> hend = min(hstart + kernel_h_, height_ + pad_h_);</span><br><span class="line">            <span class="keyword">int</span> wend = min(wstart + kernel_w_, width_ + pad_w_);</span><br><span class="line">            <span class="keyword">int</span> pool_size = (hend - hstart) * (wend - wstart);</span><br><span class="line">            hstart = max(hstart, <span class="number">0</span>);</span><br><span class="line">            wstart = max(wstart, <span class="number">0</span>);</span><br><span class="line">            hend = min(hend, height_);</span><br><span class="line">            wend = min(wend, width_);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> h = hstart; h &lt; hend; ++h) &#123;</span><br><span class="line">              <span class="keyword">for</span> (<span class="keyword">int</span> w = wstart; w &lt; wend; ++w) &#123;</span><br><span class="line">                bottom_diff[h * width_ + w] +=</span><br><span class="line">                  top_diff[ph * pooled_width_ + pw] / pool_size;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// offset</span></span><br><span class="line">        bottom_diff += bottom[<span class="number">0</span>]-&gt;offset(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">        top_diff += top[<span class="number">0</span>]-&gt;offset(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> PoolingParameter_PoolMethod_STOCHASTIC:</span><br><span class="line">    NOT_IMPLEMENTED;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Stochastic Pooling的前向传播过程示例theano代码:<a href="https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/expr/stochastic_pool.py" target="_blank" rel="noopener">stochastic_pool.py</a></p>
<p><strong>caffe中的Stochastic Pooling实现</strong>：</p>
<p>只为GPU做了代码实现，并需要与 CAFFE engine一块使用，需要在pooling_param 里边设置pool类型：STOCHASTIC ，在pooling_param 中设置<code>engine: CAFFE</code>（如果使用GPU运行，默认引擎是cuDNN）.</p>
<p>Stochastic Pooling实现代码<a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/pooling_layer.cu" target="_blank" rel="noopener">pooling_layer.cu</a>:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">StoPoolForwardTrain</span><span class="params">(..,Dtype* <span class="keyword">const</span> rand_idx,..)</span> </span>&#123;</span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">  rand_idx是随机选的pooling核上的位置比例,目前实现方式是使用如下的均匀分布产生函数生成：</span></span><br><span class="line"><span class="comment">  caffe_gpu_rng_uniform(count, Dtype(0), Dtype(1),</span></span><br><span class="line"><span class="comment">                        rand_idx_.mutable_gpu_data());</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  ...</span><br><span class="line">    Dtype cumsum = <span class="number">0.</span>;</span><br><span class="line">    <span class="keyword">const</span> Dtype* <span class="keyword">const</span> bottom_slice =</span><br><span class="line">        bottom_data + (n * channels + c) * height * width;</span><br><span class="line">    <span class="comment">// First pass: get sum</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> h = hstart; h &lt; hend; ++h) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> w = wstart; w &lt; wend; ++w) &#123;</span><br><span class="line">        cumsum += bottom_slice[h * width + w];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> thres = rand_idx[index] * cumsum;</span><br><span class="line">    <span class="comment">// Second pass: get value, and set index.</span></span><br><span class="line">    cumsum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> h = hstart; h &lt; hend; ++h) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> w = wstart; w &lt; wend; ++w) &#123;</span><br><span class="line">        cumsum += bottom_slice[h * width + w];</span><br><span class="line">        <span class="keyword">if</span> (cumsum &gt;= thres) &#123;<span class="comment">// 轮盘赌，均匀分布</span></span><br><span class="line">          rand_idx[index] = ((n * channels + c) * height + h) * width + w;</span><br><span class="line">          top_data[index] = bottom_slice[h * width + w];</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">StoPoolForwardTest</span><span class="params">(...)</span></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    Dtype cumsum = <span class="number">0.</span>;</span><br><span class="line">    Dtype cumvalues = <span class="number">0.</span>;</span><br><span class="line">    <span class="keyword">const</span> Dtype* <span class="keyword">const</span> bottom_slice =</span><br><span class="line">        bottom_data + (n * channels + c) * height * width;</span><br><span class="line">    <span class="comment">// First pass: get sum</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> h = hstart; h &lt; hend; ++h) &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> w = wstart; w &lt; wend; ++w) &#123;</span><br><span class="line">        cumsum += bottom_slice[h * width + w];<span class="comment">// 求和</span></span><br><span class="line">        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];<span class="comment">// 求平方和</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    top_data[index] = (cumsum &gt; <span class="number">0.</span>) ? cumvalues / cumsum : <span class="number">0.</span>;  </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h2><p>LeCun的“Learning Mid-Level Features For Recognition”对前两种pooling方法有比较详细的分析对比。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://makefile.ml/2017/09/28/pooling/" data-id="cjd5k7cun0002kisifp579615" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/09/28/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/09/28/pooling/">深度学习网络层之 Pooling</a>
          </li>
        
          <li>
            <a href="/2017/09/28/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 makefile<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>